[
  {
    "path": "Posts/2022-11-06-golfanalytics/",
    "title": "Data Analytics in Golf",
    "description": "I dive into how one of the most traditional sports in history is implementing data into the game.",
    "author": [
      {
        "name": "Ben Wagner",
        "url": {}
      }
    ],
    "date": "2022-11-06",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nGolf is a game of history and tradition. The sport originated during the 15th century in Scotland, but there is evidence of similar games being played around the world for many years before. However, when most people think of the game of golf, they think of green jackets, country clubs, and athletes abiding to strict rules or etiquettes. This is the aspect of the sport that drives the reputations of traditionalism and exclusivity. Even the manner in which golfers have played the game shows very little evolution or adaption. How Jack Niklaus or Arnold Palmer hit a golf ball is quite similar to today’s professionals. In many other sports, incorporating data analytics is seen as a new and taboo approach to gaining and edge on the competition. Thus, it was to my surprise that golf is slowly implementing data analytics by developing advanced data collection technologies, calculating insightful statistical metrics, and incorporation live scientists to benefit those “on tour”.\r\nTo my disbelief, the PGA Tour created the first advanced shot tracking technology in 2003. This technology, Shotlink, is still very much an aspect of today’s game as it is used in 93 events per year and has even developed exponentially over the years. Today, the tool can laser map every hole on any course in the world, so the tour is able to collect the most accurate data. I think the age of Shotlink is the most shocking feature of the tech because the PGA tour is sitting on a goldmine of golf data almost as old as I am (yes, I am a millennium baby). Another aspect of data collection that is continuing to improve in golf doesn’t involve where the ball goes, but how the golfer hits it. State of the art swing tracers are giving the edge to the golfers who pride themselves on perfection. Trackman, K-motion, and the Flightscope X2 Radar Launch Monitor each gather swing data using high-definition cameras and microwave transmissions. The tour pros all talk about the impact their club head speed has on their game. These tracers allow them to access precise metrics to the hundredth of a MPH.\r\nGolf is unique in that there is ample time between plays to measure accurate data. However, the ability to be precise has taken thousands of dollars in technology and years of research to develop. Nonetheless, sports analysts like Mark Broadie have developed metrics that are measurable and effective but workaround the limitations over the years. He developed the metric labeled strokes gained. Mark knew that looking into putting data would present useful insights. Well, even my mother knew that as I have countless memories of her telling me to “drive for show, putt for doe”. Nevertheless, Mark decided that he should be using the widely abundant and specific data to put more weight on one shot rather than another. Mark’s stat calculates the difference in number of strokes taken to complete the whole by the golfer and the average number of strokes from that golfer’s location. For example, If I have a 10-foot putt, and the average number of strokes from that same location is 2, but I sink the putt in 1, I have 1 stroke gained. Rather than comparing two golfer’s who had drastically different looks on the green. Mark thought we should compare on even grounds.\r\nAll of these innovations have progressed analytics in golf, but there is none more important than the actual athletes of the game incorporating these mechanisms into the strategy and development of the game. Athletes who see results using analytics become proponents of the statistical advantage, and after a snowball effect, the implementation of data is tour wide. Zach Johnson was the first tour player to hire an analyst to his team back in 2011. Since then, many players have followed in his footsteps including names like Bryson DeChambeau, Rory Mcllroy and Brooks Koepka. After DeChambeau focused on increasing his swing speed, using the swing tracking technology, he was able to hit shots further. By using data trends, he decided that it’d be more beneficial for him to hit it closer to the pin then making sure he hit the fairway in regulation. Today he has a top five average drive distance, and his fairway consistency is not even top 25% of all golfers.\r\nAt the beginning of this blog, I presented a hypothetical scenario to imagine asking someone to think about the game of golf and what characteristics the game exhibits. I’d say that only a very small percentage of people would think of analytics, if any at all. Yes, you could say that is synonymous to many other sports, but I found it interesting that it if not for coming to the Institute, studying analytics, and writing this blog, that I would never believe data was actually utilized within the game. It’s clear now that data is not at the forefront of the sport, but that’s not to say that in the background, analytics plays a huge role. Since my time began here at NC State, I have a unique perspective on how analytics could be used around me, especially when it revolves around something I love to do, like golf! I commend you also to look into just how your favorite things embrace the use of data because I bet you’d be surprised just like me.\r\n\r\n\r\n\r\n",
    "preview": "Posts/2022-11-06-golfanalytics/../../GolfSwing.jpg",
    "last_modified": "2022-11-06T18:23:30-05:00",
    "input_file": {}
  },
  {
    "path": "Posts/2021-12-30-twominutedrill/",
    "title": "2-Minute Drill: Understanding the Importance",
    "description": "This project dives deeper into the statistical importance of the football strategy known as the \"2-Minute Drill\". What makes a drive at the end of a half or game so tricky? How can teams effectively increase the chance of success when driving down the field with little time remaining?",
    "author": [
      {
        "name": "Ben Wagner & Anthony Palma",
        "url": {}
      }
    ],
    "date": "2021-12-30",
    "categories": [],
    "contents": "\r\nIntroduction\r\nIf you ever want to watch the greatest minds of the football world prove that they are elite, look no further than a close game with 2 minutes or less left on the clock. The “2-minute Drill” has been a staple tactic employed by teams for almost as long as the game has been around. Wikipedia defines this style of hurry-up offense as a “high-pressure and fast-paced situational strategy where a team will focus on clock management, maximizing the number of plays available for a scoring attempt before a half (or game) expires.” When teams perform the 2-minute drill, you should expect them to manage the clock using timeouts and plays that eliminate a running clock. You can expect a two minute drill drive in about 1 in 5 games, so it makes sense why these drives are so important.\r\nHere you can see the number of drives that began under 2 minutes and 30 seconds left in the first or second halves from 2018 to today. As you can see, it is not too often that a team successfully completes a drive by scoring points. Just more than 21% of the time, teams have scored at least 3 points by kicking a field goal. While only 10% of the time, teams have reached the endzone for 6.\r\n\r\n\r\n\r\nTwo Minute Drill Success Rates\r\n\r\n\r\n\r\nNumber of TD’s Scored\r\n\r\n\r\nNumber of FG’s Scored\r\n\r\n\r\nNumber of Drives\r\n\r\n\r\nTD Success Rate\r\n\r\n\r\nFG Success Rate\r\n\r\n\r\n45\r\n\r\n\r\n65\r\n\r\n\r\n332\r\n\r\n\r\n0.1355422\r\n\r\n\r\n0.1957831\r\n\r\n\r\n\r\n\r\n\r\n2-minute Drills in the last 4 Seasons\r\n\r\n\r\n\r\nseason\r\n\r\n\r\nNumber of TD’s Scored\r\n\r\n\r\nNumber of FG’s Scored\r\n\r\n\r\nNumber of Drives\r\n\r\n\r\nTD Success Rate\r\n\r\n\r\nFG Success Rate\r\n\r\n\r\n2018\r\n\r\n\r\n8\r\n\r\n\r\n18\r\n\r\n\r\n78\r\n\r\n\r\n0.1025641\r\n\r\n\r\n0.2307692\r\n\r\n\r\n2019\r\n\r\n\r\n14\r\n\r\n\r\n14\r\n\r\n\r\n88\r\n\r\n\r\n0.1590909\r\n\r\n\r\n0.1590909\r\n\r\n\r\n2020\r\n\r\n\r\n15\r\n\r\n\r\n18\r\n\r\n\r\n88\r\n\r\n\r\n0.1704545\r\n\r\n\r\n0.2045455\r\n\r\n\r\n2021\r\n\r\n\r\n8\r\n\r\n\r\n15\r\n\r\n\r\n78\r\n\r\n\r\n0.1025641\r\n\r\n\r\n0.1923077\r\n\r\n\r\nNow why should we take a look at this specific aspect of football? A successful 2-minute drill could have massive impacts on the probability of the team winning the game. Let me remind you of week 10 in 2020. The Buffalo Bills drove the length of the field, managing their own two minute drill. Josh Allen and the Bills capped off the drive by finding the endzone when Allen slung a beautiful 40 yard dot to his favorite target Stefon Diggs with just over 30 seconds left in the game. There was a 90% probability that the Bills had just secured the win, but the Cardinals had other plans.\r\nIn a mere 3 plays, the Cardinals marched down to the 43 yard line in Bills territory. The rest will go down as one of the greatest plays in NFL history. Murray scrambles out of the pocket and heaves one down the field to a triple teamed DeAndre Hopkins who reaches up and snags the Bills hopes. Although a little luck was involved in this drive, the Cardinals effectively managed the amount of time they were given and won the game despite the statistics. That is what a great two minute drill drive can do for a team any given week.\r\n\r\n\r\n\r\nHere we can see the quarterbacks that have the best rate of scoring either a field goal or a touchdown over the past 4 seasons. Those closer to the top right show us that they are efficient when the clock strike below 2.\r\n\r\n\r\n\r\nThere are few positions that require such a responsibility as an NFL Quarterback. The only other position in team sports that stand out to me is an MLB Pitcher. As you can see from these two plots, the better the QB plays that week, the higher probability the team has at completing that 2-minute drill with new points on the board. Every decision a Quarterback makes during those final 2 minutes could have massive repercussions, so its important the team has the right guy for the job.\r\n\r\n\r\n\r\n\r\n\r\n\r\nWe will explore the relationship between football stats in our set and the score outcome of two minute drills from the 2018 - 2021 seasons.\r\nWe will do so using primarily a logistic regression and will also explore random forests and boosted trees to ensure we explored all other options.\r\nModeling\r\nLasso Logisitic Regression\r\nOur primary model we will be focusing on is a Logistic Regression. This model specializes in predicting probabilities of our outcome, not just the outcome. That way with this model, we can not only see whether it predicts a score or not but we can see and have access to the probabilities it used to predict the outcome.\r\nWhen creating this model, we will be using a Lasso approach which stands for “Least Absolute Shrinkage and Selection Operator.” In short, this type of modeling selects variables and their impact size while taking into account maximizing the accuracy and interpretability of the model.\r\n\r\n\r\n\r\nBelow are the results of our 5 fold cross validation from our modeling. In short, we separated the data into two groups: one to build the model off (75%) and the other to test and see how well the model works.\r\nEach fold is using a different part of the 75% of the training set to build the model and testing upon the rest. This is done over and over and over to amass tons of data on the best logistic regression model.\r\n\r\n\r\n\r\nLasso Log Reg Models Performance\r\n\r\n\r\n\r\nid\r\n\r\n\r\npenalty\r\n\r\n\r\n.metric\r\n\r\n\r\n.estimator\r\n\r\n\r\n.estimate\r\n\r\n\r\n.config\r\n\r\n\r\nFold1\r\n\r\n\r\n0.0000000\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.7400000\r\n\r\n\r\nPreprocessor1_Model01\r\n\r\n\r\nFold1\r\n\r\n\r\n0.0000000\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.7400000\r\n\r\n\r\nPreprocessor1_Model02\r\n\r\n\r\nFold1\r\n\r\n\r\n0.0000000\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.7400000\r\n\r\n\r\nPreprocessor1_Model03\r\n\r\n\r\nFold1\r\n\r\n\r\n0.0000002\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.7400000\r\n\r\n\r\nPreprocessor1_Model04\r\n\r\n\r\nFold1\r\n\r\n\r\n0.0000028\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.7400000\r\n\r\n\r\nPreprocessor1_Model05\r\n\r\n\r\nFold1\r\n\r\n\r\n0.0000359\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.7400000\r\n\r\n\r\nPreprocessor1_Model06\r\n\r\n\r\nFold1\r\n\r\n\r\n0.0004642\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.7400000\r\n\r\n\r\nPreprocessor1_Model07\r\n\r\n\r\nFold1\r\n\r\n\r\n0.0059948\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.7400000\r\n\r\n\r\nPreprocessor1_Model08\r\n\r\n\r\nFold1\r\n\r\n\r\n0.0774264\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.7200000\r\n\r\n\r\nPreprocessor1_Model09\r\n\r\n\r\nFold1\r\n\r\n\r\n1.0000000\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.4400000\r\n\r\n\r\nPreprocessor1_Model10\r\n\r\n\r\nFold2\r\n\r\n\r\n0.0000000\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.8600000\r\n\r\n\r\nPreprocessor1_Model01\r\n\r\n\r\nFold2\r\n\r\n\r\n0.0000000\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.8600000\r\n\r\n\r\nPreprocessor1_Model02\r\n\r\n\r\nFold2\r\n\r\n\r\n0.0000000\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.8600000\r\n\r\n\r\nPreprocessor1_Model03\r\n\r\n\r\nFold2\r\n\r\n\r\n0.0000002\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.8600000\r\n\r\n\r\nPreprocessor1_Model04\r\n\r\n\r\nFold2\r\n\r\n\r\n0.0000028\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.8600000\r\n\r\n\r\nPreprocessor1_Model05\r\n\r\n\r\nFold2\r\n\r\n\r\n0.0000359\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.8600000\r\n\r\n\r\nPreprocessor1_Model06\r\n\r\n\r\nFold2\r\n\r\n\r\n0.0004642\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.8600000\r\n\r\n\r\nPreprocessor1_Model07\r\n\r\n\r\nFold2\r\n\r\n\r\n0.0059948\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.8400000\r\n\r\n\r\nPreprocessor1_Model08\r\n\r\n\r\nFold2\r\n\r\n\r\n0.0774264\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.8200000\r\n\r\n\r\nPreprocessor1_Model09\r\n\r\n\r\nFold2\r\n\r\n\r\n1.0000000\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.6600000\r\n\r\n\r\nPreprocessor1_Model10\r\n\r\n\r\nFold3\r\n\r\n\r\n0.0000000\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.8800000\r\n\r\n\r\nPreprocessor1_Model01\r\n\r\n\r\nFold3\r\n\r\n\r\n0.0000000\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.8800000\r\n\r\n\r\nPreprocessor1_Model02\r\n\r\n\r\nFold3\r\n\r\n\r\n0.0000000\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.8800000\r\n\r\n\r\nPreprocessor1_Model03\r\n\r\n\r\nFold3\r\n\r\n\r\n0.0000002\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.8800000\r\n\r\n\r\nPreprocessor1_Model04\r\n\r\n\r\nFold3\r\n\r\n\r\n0.0000028\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.8800000\r\n\r\n\r\nPreprocessor1_Model05\r\n\r\n\r\nFold3\r\n\r\n\r\n0.0000359\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.8800000\r\n\r\n\r\nPreprocessor1_Model06\r\n\r\n\r\nFold3\r\n\r\n\r\n0.0004642\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.8800000\r\n\r\n\r\nPreprocessor1_Model07\r\n\r\n\r\nFold3\r\n\r\n\r\n0.0059948\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.8800000\r\n\r\n\r\nPreprocessor1_Model08\r\n\r\n\r\nFold3\r\n\r\n\r\n0.0774264\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.8800000\r\n\r\n\r\nPreprocessor1_Model09\r\n\r\n\r\nFold3\r\n\r\n\r\n1.0000000\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.7800000\r\n\r\n\r\nPreprocessor1_Model10\r\n\r\n\r\nFold4\r\n\r\n\r\n0.0000000\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.8163265\r\n\r\n\r\nPreprocessor1_Model01\r\n\r\n\r\nFold4\r\n\r\n\r\n0.0000000\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.8163265\r\n\r\n\r\nPreprocessor1_Model02\r\n\r\n\r\nFold4\r\n\r\n\r\n0.0000000\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.8163265\r\n\r\n\r\nPreprocessor1_Model03\r\n\r\n\r\nFold4\r\n\r\n\r\n0.0000002\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.8163265\r\n\r\n\r\nPreprocessor1_Model04\r\n\r\n\r\nFold4\r\n\r\n\r\n0.0000028\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.8163265\r\n\r\n\r\nPreprocessor1_Model05\r\n\r\n\r\nFold4\r\n\r\n\r\n0.0000359\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.8163265\r\n\r\n\r\nPreprocessor1_Model06\r\n\r\n\r\nFold4\r\n\r\n\r\n0.0004642\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.8163265\r\n\r\n\r\nPreprocessor1_Model07\r\n\r\n\r\nFold4\r\n\r\n\r\n0.0059948\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.8367347\r\n\r\n\r\nPreprocessor1_Model08\r\n\r\n\r\nFold4\r\n\r\n\r\n0.0774264\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.8163265\r\n\r\n\r\nPreprocessor1_Model09\r\n\r\n\r\nFold4\r\n\r\n\r\n1.0000000\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.3469388\r\n\r\n\r\nPreprocessor1_Model10\r\n\r\n\r\nFold5\r\n\r\n\r\n0.0000000\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.7755102\r\n\r\n\r\nPreprocessor1_Model01\r\n\r\n\r\nFold5\r\n\r\n\r\n0.0000000\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.7755102\r\n\r\n\r\nPreprocessor1_Model02\r\n\r\n\r\nFold5\r\n\r\n\r\n0.0000000\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.7755102\r\n\r\n\r\nPreprocessor1_Model03\r\n\r\n\r\nFold5\r\n\r\n\r\n0.0000002\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.7755102\r\n\r\n\r\nPreprocessor1_Model04\r\n\r\n\r\nFold5\r\n\r\n\r\n0.0000028\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.7755102\r\n\r\n\r\nPreprocessor1_Model05\r\n\r\n\r\nFold5\r\n\r\n\r\n0.0000359\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.7755102\r\n\r\n\r\nPreprocessor1_Model06\r\n\r\n\r\nFold5\r\n\r\n\r\n0.0004642\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.7755102\r\n\r\n\r\nPreprocessor1_Model07\r\n\r\n\r\nFold5\r\n\r\n\r\n0.0059948\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.7755102\r\n\r\n\r\nPreprocessor1_Model08\r\n\r\n\r\nFold5\r\n\r\n\r\n0.0774264\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.7346939\r\n\r\n\r\nPreprocessor1_Model09\r\n\r\n\r\nFold5\r\n\r\n\r\n1.0000000\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.6938776\r\n\r\n\r\nPreprocessor1_Model10\r\n\r\n\r\nThose different folds and their resulting accuracies were also used to try out different penalty parameters for lasso, which in this case helps to determine what the insignificance cut off is for throwing out “indeterminate” variables. The accuracies appear to be quite high for most penalties.\r\n\r\n\r\n\r\nThe goal of modeling is to extract our most accurate model and its associated penalty so that we can finalize the model and interpret some meaning.\r\n\r\n\r\n\r\nLasso Log Reg Best Accuracy Models\r\n\r\n\r\n\r\npenalty\r\n\r\n\r\n.metric\r\n\r\n\r\n.estimator\r\n\r\n\r\nmean\r\n\r\n\r\nn\r\n\r\n\r\nstd_err\r\n\r\n\r\n.config\r\n\r\n\r\n0.0059948\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.8144490\r\n\r\n\r\n5\r\n\r\n\r\n0.0250053\r\n\r\n\r\nPreprocessor1_Model08\r\n\r\n\r\n0.0000000\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.8143673\r\n\r\n\r\n5\r\n\r\n\r\n0.0259174\r\n\r\n\r\nPreprocessor1_Model01\r\n\r\n\r\n0.0000000\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.8143673\r\n\r\n\r\n5\r\n\r\n\r\n0.0259174\r\n\r\n\r\nPreprocessor1_Model02\r\n\r\n\r\n0.0000000\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.8143673\r\n\r\n\r\n5\r\n\r\n\r\n0.0259174\r\n\r\n\r\nPreprocessor1_Model03\r\n\r\n\r\n0.0000002\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.8143673\r\n\r\n\r\n5\r\n\r\n\r\n0.0259174\r\n\r\n\r\nPreprocessor1_Model04\r\n\r\n\r\nThe best penalty which maximized our accuracy is below and will be directly input to finalize our model.\r\n\r\n\r\n\r\nLasso Log Reg Best Model\r\n\r\n\r\n\r\npenalty\r\n\r\n\r\n.config\r\n\r\n\r\n0.0059948\r\n\r\n\r\nPreprocessor1_Model08\r\n\r\n\r\n\r\n== Workflow ==========================================================\r\nPreprocessor: Recipe\r\nModel: logistic_reg()\r\n\r\n-- Preprocessor ------------------------------------------------------\r\n3 Recipe Steps\r\n\r\n* step_upsample()\r\n* step_dummy()\r\n* step_normalize()\r\n\r\n-- Model -------------------------------------------------------------\r\nLogistic Regression Model Specification (classification)\r\n\r\nMain Arguments:\r\n  penalty = 0.00599484250318942\r\n  mixture = 1\r\n\r\nComputational engine: glmnet \r\n\r\nNow for some interpretation and meaning….below we have the coefficients of each variable from the logistic regression. While normally in log odds form, we exponentiated the variable coefficients to now have them in an odds and odds ratio form, a much more interpretable style.\r\n\r\n\r\n\r\nLasso Log Reg Finalized Model Coefficients\r\n\r\n\r\n\r\nterm\r\n\r\n\r\nestimate\r\n\r\n\r\n(Intercept)\r\n\r\n\r\n0.8656817\r\n\r\n\r\nqbr_raw\r\n\r\n\r\n1.2605909\r\n\r\n\r\nqbr_total\r\n\r\n\r\n1.0000000\r\n\r\n\r\npass_tot_yds\r\n\r\n\r\n0.8578689\r\n\r\n\r\ntot_yds\r\n\r\n\r\n1.0207665\r\n\r\n\r\nydsnet\r\n\r\n\r\n13.4394059\r\n\r\n\r\nrush_yds_tot\r\n\r\n\r\n1.0000000\r\n\r\n\r\ncompletion_perc\r\n\r\n\r\n0.9992010\r\n\r\n\r\nrun_plays\r\n\r\n\r\n1.0000000\r\n\r\n\r\npass_plays\r\n\r\n\r\n1.0000000\r\n\r\n\r\ndrive_yards_penalized\r\n\r\n\r\n1.2622421\r\n\r\n\r\ndrive_game_clock_start\r\n\r\n\r\n0.9124922\r\n\r\n\r\nyards_to_go_start\r\n\r\n\r\n0.2424558\r\n\r\n\r\nAbove, you can see the logistic regression model output for all of our variables and their coefficients.\r\nGenerally, if a coefficient is above one (1) it shows a positive impact upon the likelihood of a two minute drill drive resulting in success.\r\nA coefficient value of one (1) indicates no change. And a coefficient value of less than one (1) means an increase in that variable results in a lesser chance of scoring.\r\nVariables that positively impact the likelihood of scoring:\r\nqbr raw\r\ntotal yards\r\nyards net – LARGEST positive impact upon score outcome\r\ntotal rush yards\r\ndrive yards penalized\r\nVariables that negatively impact the likelihood of scoring:\r\ntotal pass yards\r\nrun plays\r\ndrive game clock\r\nyards to go – LARGEST negative impact upon score outcome\r\nYou can see the result of yards net having the largest impact upon score outcome in the Variable Importance plot below.\r\nThe massive positive impact upon score outcome is nearly matched by the massive negative impact upon scoring derived from the yards to go variable, which had the largest negatively correlated variable coefficient above.\r\nYou may notice more variables included on the VI plot than coefficients above. That is because the lasso approach regularized and shrunk the least contributing variables coefficients to basically zero, as seen with these additional variables being quite unimportant to predicting score outcome.\r\n\r\n\r\n\r\nOur best fitting model was quite well in prediction with a strong 86% accuracy. Additionally, its ROC AUC, or the area under the curve, is quite high as well at 88% denoting a high level of confidence that the model will be able to distinguish between the score and no score class (88% confident to be exact).\r\n\r\n\r\n\r\nLasso Log Reg Best Model Accuracy and ROC\r\n\r\n\r\n\r\n.metric\r\n\r\n\r\n.estimator\r\n\r\n\r\n.estimate\r\n\r\n\r\n.config\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.7976190\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\nroc_auc\r\n\r\n\r\nbinary\r\n\r\n\r\n0.8928571\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\nWe can dig a bit deeper into how well the model predicts by looking at how the model predicts on certain specific instances from our testing set. Along with predicted class and actual class, you can also see the probability associated with each class leading to the prediction, with 50% as the threshold.\r\n\r\n\r\n\r\nLasso Log Reg Model Predictions\r\n\r\n\r\n\r\nid\r\n\r\n\r\n.pred_0\r\n\r\n\r\n.pred_1\r\n\r\n\r\n.row\r\n\r\n\r\n.pred_class\r\n\r\n\r\nscore\r\n\r\n\r\n.config\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.9640260\r\n\r\n\r\n0.0359740\r\n\r\n\r\n2\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.9732424\r\n\r\n\r\n0.0267576\r\n\r\n\r\n4\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.5859085\r\n\r\n\r\n0.4140915\r\n\r\n\r\n7\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.9437801\r\n\r\n\r\n0.0562199\r\n\r\n\r\n8\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.0770452\r\n\r\n\r\n0.9229548\r\n\r\n\r\n11\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.2627125\r\n\r\n\r\n0.7372875\r\n\r\n\r\n18\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.9900497\r\n\r\n\r\n0.0099503\r\n\r\n\r\n24\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.8665691\r\n\r\n\r\n0.1334309\r\n\r\n\r\n25\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.0437545\r\n\r\n\r\n0.9562455\r\n\r\n\r\n36\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.4207155\r\n\r\n\r\n0.5792845\r\n\r\n\r\n48\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.2940966\r\n\r\n\r\n0.7059034\r\n\r\n\r\n49\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.9339000\r\n\r\n\r\n0.0661000\r\n\r\n\r\n50\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.0703400\r\n\r\n\r\n0.9296600\r\n\r\n\r\n63\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.0453868\r\n\r\n\r\n0.9546132\r\n\r\n\r\n64\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.8800837\r\n\r\n\r\n0.1199163\r\n\r\n\r\n65\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.9410727\r\n\r\n\r\n0.0589273\r\n\r\n\r\n68\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.5638419\r\n\r\n\r\n0.4361581\r\n\r\n\r\n71\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.9879537\r\n\r\n\r\n0.0120463\r\n\r\n\r\n74\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.9095917\r\n\r\n\r\n0.0904083\r\n\r\n\r\n78\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.9986199\r\n\r\n\r\n0.0013801\r\n\r\n\r\n82\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.3986212\r\n\r\n\r\n0.6013788\r\n\r\n\r\n84\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.5877888\r\n\r\n\r\n0.4122112\r\n\r\n\r\n86\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.2152162\r\n\r\n\r\n0.7847838\r\n\r\n\r\n90\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.0723483\r\n\r\n\r\n0.9276517\r\n\r\n\r\n91\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.0448962\r\n\r\n\r\n0.9551038\r\n\r\n\r\n102\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.1849738\r\n\r\n\r\n0.8150262\r\n\r\n\r\n107\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.7253247\r\n\r\n\r\n0.2746753\r\n\r\n\r\n108\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.9991837\r\n\r\n\r\n0.0008163\r\n\r\n\r\n111\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.9337336\r\n\r\n\r\n0.0662664\r\n\r\n\r\n121\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.4673016\r\n\r\n\r\n0.5326984\r\n\r\n\r\n122\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.6597570\r\n\r\n\r\n0.3402430\r\n\r\n\r\n129\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.9602456\r\n\r\n\r\n0.0397544\r\n\r\n\r\n133\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.0705417\r\n\r\n\r\n0.9294583\r\n\r\n\r\n141\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.9722851\r\n\r\n\r\n0.0277149\r\n\r\n\r\n142\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.9392277\r\n\r\n\r\n0.0607723\r\n\r\n\r\n149\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.0472814\r\n\r\n\r\n0.9527186\r\n\r\n\r\n151\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.1221500\r\n\r\n\r\n0.8778500\r\n\r\n\r\n152\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.4002076\r\n\r\n\r\n0.5997924\r\n\r\n\r\n154\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.9738012\r\n\r\n\r\n0.0261988\r\n\r\n\r\n155\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.8785670\r\n\r\n\r\n0.1214330\r\n\r\n\r\n160\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.1004549\r\n\r\n\r\n0.8995451\r\n\r\n\r\n164\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.8652678\r\n\r\n\r\n0.1347322\r\n\r\n\r\n169\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.9832480\r\n\r\n\r\n0.0167520\r\n\r\n\r\n173\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.0636686\r\n\r\n\r\n0.9363314\r\n\r\n\r\n177\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.9698480\r\n\r\n\r\n0.0301520\r\n\r\n\r\n180\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.9658407\r\n\r\n\r\n0.0341593\r\n\r\n\r\n182\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.6698217\r\n\r\n\r\n0.3301783\r\n\r\n\r\n189\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.1771437\r\n\r\n\r\n0.8228563\r\n\r\n\r\n192\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.4492787\r\n\r\n\r\n0.5507213\r\n\r\n\r\n197\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.9914737\r\n\r\n\r\n0.0085263\r\n\r\n\r\n201\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.6475561\r\n\r\n\r\n0.3524439\r\n\r\n\r\n205\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.7322905\r\n\r\n\r\n0.2677095\r\n\r\n\r\n206\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.5184468\r\n\r\n\r\n0.4815532\r\n\r\n\r\n207\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.9936359\r\n\r\n\r\n0.0063641\r\n\r\n\r\n211\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.5370902\r\n\r\n\r\n0.4629098\r\n\r\n\r\n214\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.8521797\r\n\r\n\r\n0.1478203\r\n\r\n\r\n216\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.4968382\r\n\r\n\r\n0.5031618\r\n\r\n\r\n222\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.8277949\r\n\r\n\r\n0.1722051\r\n\r\n\r\n223\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.9768771\r\n\r\n\r\n0.0231229\r\n\r\n\r\n224\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.2863538\r\n\r\n\r\n0.7136462\r\n\r\n\r\n236\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.8879658\r\n\r\n\r\n0.1120342\r\n\r\n\r\n239\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.0369594\r\n\r\n\r\n0.9630406\r\n\r\n\r\n241\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.0546965\r\n\r\n\r\n0.9453035\r\n\r\n\r\n243\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.0469425\r\n\r\n\r\n0.9530575\r\n\r\n\r\n254\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.5783051\r\n\r\n\r\n0.4216949\r\n\r\n\r\n260\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.9562602\r\n\r\n\r\n0.0437398\r\n\r\n\r\n263\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.2790645\r\n\r\n\r\n0.7209355\r\n\r\n\r\n266\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.9958274\r\n\r\n\r\n0.0041726\r\n\r\n\r\n270\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.9864895\r\n\r\n\r\n0.0135105\r\n\r\n\r\n273\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.9024911\r\n\r\n\r\n0.0975089\r\n\r\n\r\n279\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.8996745\r\n\r\n\r\n0.1003255\r\n\r\n\r\n285\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.9929761\r\n\r\n\r\n0.0070239\r\n\r\n\r\n291\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.9737378\r\n\r\n\r\n0.0262622\r\n\r\n\r\n295\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.0857039\r\n\r\n\r\n0.9142961\r\n\r\n\r\n298\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.9614318\r\n\r\n\r\n0.0385682\r\n\r\n\r\n299\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.9768009\r\n\r\n\r\n0.0231991\r\n\r\n\r\n304\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.5208938\r\n\r\n\r\n0.4791062\r\n\r\n\r\n318\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.9832017\r\n\r\n\r\n0.0167983\r\n\r\n\r\n319\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.1531777\r\n\r\n\r\n0.8468223\r\n\r\n\r\n321\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.0920780\r\n\r\n\r\n0.9079220\r\n\r\n\r\n325\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.8585376\r\n\r\n\r\n0.1414624\r\n\r\n\r\n327\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.0401372\r\n\r\n\r\n0.9598628\r\n\r\n\r\n329\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.1329296\r\n\r\n\r\n0.8670704\r\n\r\n\r\n330\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\ntrain/test split\r\n\r\n\r\n0.1298200\r\n\r\n\r\n0.8701800\r\n\r\n\r\n332\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\nWe can also look at our predictions in the aggregate with a matrixed table.\r\n\r\n          Truth\r\nPrediction  0  1\r\n         0 45 11\r\n         1  6 22\r\n\r\nOther important model metrics are laid out in the table below, with these metrics also proving how well the model predicts.\r\nMetrics to Note:\r\nSens = Sensitivity – Ratio between how much was classified as a score to how much was actually a score of that Spec = Specificity – Ratio between how much was classified as not a score to how much was actually not a score of that Precision – Ratio of how much was correctly classified as a score out of all scores  F meas – Indicates classification strength\r\n\r\n\r\n\r\nLasso Log Reg Finalized Model Metrics\r\n\r\n\r\n\r\n.metric\r\n\r\n\r\n.estimator\r\n\r\n\r\n.estimate\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.7976190\r\n\r\n\r\nsens\r\n\r\n\r\nbinary\r\n\r\n\r\n0.8035714\r\n\r\n\r\nspec\r\n\r\n\r\nbinary\r\n\r\n\r\n0.7857143\r\n\r\n\r\nprecision\r\n\r\n\r\nbinary\r\n\r\n\r\n0.8823529\r\n\r\n\r\nf_meas\r\n\r\n\r\nbinary\r\n\r\n\r\n0.8411215\r\n\r\n\r\nThe relationship between sensitivity and specificity can be seen below The dotted line in the middle is what the roc curve would look like if the model predicted based on random 50/50 choice. The further our line is away from this line, the better the model is as we see.\r\n\r\n\r\n\r\nBelow is a lay out of what probabilities in this model resulted in certain predictions. You can see the impact of the 0.5 threshold as hardly any 1 predictions occur where the probability of being 1 is less than 0.5 but it jumps after crossing 0.5.\r\n\r\n\r\n\r\nWith this great and accurate model, we put it to the test with a recent drive from the College Football playoffs.\r\nPredicting with our model…\r\n(2) Clemson vs (1) Alabama: Jan 9, 2017\r\n31-28 Alabama with 2:01 left in 4Q Clemson ball on Clem 32, 68 yards to go to win the game Clemson ends up driving all 68 yards for a Deshaun Watson pass to Hunter Renfrow for a TD to win the game with 1 second left.\r\n\r\n\r\n\r\nHow does our best model, the logistic regression predict this?\r\nData was obtained from ESPN’s cache of play by play …. some unavailable data at the time like game qbr was replaced with an average of that metric for that player for the season\r\n\r\n\r\n\r\nLasso Log Reg New QB Entry\r\n\r\n\r\n\r\nqbr_raw\r\n\r\n\r\nqbr_total\r\n\r\n\r\npass_tot_yds\r\n\r\n\r\ntot_yds\r\n\r\n\r\nydsnet\r\n\r\n\r\nrush_yds_tot\r\n\r\n\r\ncompletion_perc\r\n\r\n\r\nrun_plays\r\n\r\n\r\npass_plays\r\n\r\n\r\ndrive_yards_penalized\r\n\r\n\r\ndrive_game_clock_start\r\n\r\n\r\nyards_to_go_start\r\n\r\n\r\nscore\r\n\r\n\r\n104.5\r\n\r\n\r\n104.5\r\n\r\n\r\n60\r\n\r\n\r\n68\r\n\r\n\r\n68\r\n\r\n\r\n1\r\n\r\n\r\n0.6667\r\n\r\n\r\n1\r\n\r\n\r\n9\r\n\r\n\r\n7\r\n\r\n\r\n127\r\n\r\n\r\n68\r\n\r\n\r\n1\r\n\r\n\r\n\r\n\r\n\r\nLasso Log Reg New QB Entry - Prediction\r\n\r\n\r\n\r\n.pred_class\r\n\r\n\r\n1\r\n\r\n\r\nYou can see our model predicts a score correctly!\r\nFor the reasons above, we feel very comfortable with the highly accurate and interpretable Lasso Log Regression. To ensure we weren’t missing a home run with any other model types, we also explored modeling with two different decision tree applications.\r\nDecision Tree – Random Forest\r\nThe next model we will briefly touch upon is a decision tree, specifically a random forest model. This model type uses a series of decision branch offs which work to split based upon significant variables.\r\n\r\n\r\n\r\n\r\n\r\n\r\nPerforming a similar process as above, we were able to pull our overall accuracy from our best model and our ROC AUC as well. We can see a decently high 80% accuracy rate and 88% ROC AUC.\r\n\r\n\r\n\r\nDecision Tree Random Forest Best Model\r\n\r\n\r\n\r\nmtry\r\n\r\n\r\nmin_n\r\n\r\n\r\n.metric\r\n\r\n\r\n.estimator\r\n\r\n\r\nmean\r\n\r\n\r\nn\r\n\r\n\r\nstd_err\r\n\r\n\r\n.config\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.7943673\r\n\r\n\r\n5\r\n\r\n\r\n0.0273602\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\nroc_auc\r\n\r\n\r\nbinary\r\n\r\n\r\n0.8706435\r\n\r\n\r\n5\r\n\r\n\r\n0.0105285\r\n\r\n\r\nPreprocessor1_Model1\r\n\r\n\r\n\r\n\r\n\r\nDecision Tree Random Forest Best Model\r\n\r\n\r\n\r\nmtry\r\n\r\n\r\nmin_n\r\n\r\n\r\n.config\r\n\r\n\r\n12\r\n\r\n\r\n2\r\n\r\n\r\nPreprocessor1_Model3\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nYou can see a goofier looking accuracy chart as the tuning parameters here are related to how the tree decides cut offs including minimum number of trees. You can see from our selected best model that the min_n optimized value of 2 and mtry value of 6 represent the peak accuracy values.\r\nThis model was lacking in interpretability in a sense that we desired a more standard regression output as provided by a logistic regression above.\r\nYet, this is another model option for future work.\r\nBoosted Decision Tree\r\nSimilar to the previous model we fit, a Boosted Decision tree creates a series of splits, or branches. The only difference with this type of model is that each tree is NOT independent of the others. This Boosted model takes information from the prior trees, and uses it to create the next splits.\r\n\r\n\r\n\r\nAfter finding the best fit boosted model, we can again pull the overall accuracy of the model. Our boosted model shows an accuracy rate of 80.5% with an ROC AUC of 89%. This is almost exactly the same as the random forest.\r\n\r\n\r\n\r\nDecision Tree Boosted Best Accuracy Models\r\n\r\n\r\n\r\nlearn_rate\r\n\r\n\r\n.metric\r\n\r\n\r\n.estimator\r\n\r\n\r\nmean\r\n\r\n\r\nn\r\n\r\n\r\nstd_err\r\n\r\n\r\n.config\r\n\r\n\r\n0.1\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.8062857\r\n\r\n\r\n5\r\n\r\n\r\n0.0268504\r\n\r\n\r\nPreprocessor1_Model10\r\n\r\n\r\n0.1\r\n\r\n\r\nroc_auc\r\n\r\n\r\nbinary\r\n\r\n\r\n0.8793612\r\n\r\n\r\n5\r\n\r\n\r\n0.0221081\r\n\r\n\r\nPreprocessor1_Model10\r\n\r\n\r\n\r\n\r\n\r\nDecision Tree Boosted Best Model\r\n\r\n\r\n\r\nlearn_rate\r\n\r\n\r\n.config\r\n\r\n\r\n0.001\r\n\r\n\r\nPreprocessor1_Model08\r\n\r\n\r\nAgain, like the Random Forest model we created, the Boosted model is hard to interpret and actually less accurate than the logistic regression we fit first. It would be much more beneficial to have a model of which we can view each variable and understand how that affects the final prediction. The boosted model lacks that trait.\r\nComparing Model Performance\r\nWe will take a peak at some overall statistics from all three models to compare them across the board.\r\n\r\n\r\n\r\nLasso Models Performance\r\n\r\n\r\n\r\npenalty\r\n\r\n\r\naccuracy\r\n\r\n\r\ntrue_neg_rate\r\n\r\n\r\ntrue_pos_rate\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.8143673\r\n\r\n\r\n0.8264184\r\n\r\n\r\n0.7889127\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.8143673\r\n\r\n\r\n0.8264184\r\n\r\n\r\n0.7889127\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.8143673\r\n\r\n\r\n0.8264184\r\n\r\n\r\n0.7889127\r\n\r\n\r\n0.0000002\r\n\r\n\r\n0.8143673\r\n\r\n\r\n0.8264184\r\n\r\n\r\n0.7889127\r\n\r\n\r\n0.0000028\r\n\r\n\r\n0.8143673\r\n\r\n\r\n0.8264184\r\n\r\n\r\n0.7889127\r\n\r\n\r\n0.0000359\r\n\r\n\r\n0.8143673\r\n\r\n\r\n0.8264184\r\n\r\n\r\n0.7889127\r\n\r\n\r\n0.0004642\r\n\r\n\r\n0.8143673\r\n\r\n\r\n0.8264184\r\n\r\n\r\n0.7889127\r\n\r\n\r\n0.0059948\r\n\r\n\r\n0.8144490\r\n\r\n\r\n0.8203578\r\n\r\n\r\n0.8006774\r\n\r\n\r\n0.0774264\r\n\r\n\r\n0.7942041\r\n\r\n\r\n0.7942679\r\n\r\n\r\n0.7824955\r\n\r\n\r\n1.0000000\r\n\r\n\r\n0.5841633\r\n\r\n\r\n0.6000000\r\n\r\n\r\n0.4000000\r\n\r\n\r\nLog Reg Avg Accuracy = 0.803129\r\n\r\n\r\n\r\nDecision Tree – Random Forest Models Performance\r\n\r\n\r\n\r\nmtry\r\n\r\n\r\nmin_n\r\n\r\n\r\naccuracy\r\n\r\n\r\ntrue_neg_rate\r\n\r\n\r\ntrue_pos_rate\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\n0.7943673\r\n\r\n\r\n0.8609041\r\n\r\n\r\n0.6609289\r\n\r\n\r\n1\r\n\r\n\r\n21\r\n\r\n\r\n0.7863673\r\n\r\n\r\n0.8308308\r\n\r\n\r\n0.6949158\r\n\r\n\r\n1\r\n\r\n\r\n40\r\n\r\n\r\n0.7621224\r\n\r\n\r\n0.8016254\r\n\r\n\r\n0.6844583\r\n\r\n\r\n6\r\n\r\n\r\n2\r\n\r\n\r\n0.7982041\r\n\r\n\r\n0.8659122\r\n\r\n\r\n0.6545060\r\n\r\n\r\n6\r\n\r\n\r\n21\r\n\r\n\r\n0.7822041\r\n\r\n\r\n0.8422045\r\n\r\n\r\n0.6573906\r\n\r\n\r\n6\r\n\r\n\r\n40\r\n\r\n\r\n0.7943673\r\n\r\n\r\n0.8185828\r\n\r\n\r\n0.7414027\r\n\r\n\r\n12\r\n\r\n\r\n2\r\n\r\n\r\n0.8141224\r\n\r\n\r\n0.8657339\r\n\r\n\r\n0.7130782\r\n\r\n\r\n12\r\n\r\n\r\n21\r\n\r\n\r\n0.8022041\r\n\r\n\r\n0.8475176\r\n\r\n\r\n0.7045312\r\n\r\n\r\n12\r\n\r\n\r\n40\r\n\r\n\r\n0.7942041\r\n\r\n\r\n0.8110850\r\n\r\n\r\n0.7539027\r\n\r\n\r\nRandom Forest Avg Accuracy = 0.8103007\r\n\r\n\r\n\r\nDecision Tree – Boosted Tree Models Performance\r\n\r\n\r\n\r\nlearn_rate\r\n\r\n\r\naccuracy\r\n\r\n\r\ntrue_neg_rate\r\n\r\n\r\ntrue_pos_rate\r\n\r\n\r\n0e+00\r\n\r\n\r\n0.6690612\r\n\r\n\r\n1.0000000\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0e+00\r\n\r\n\r\n0.7620408\r\n\r\n\r\n0.7016995\r\n\r\n\r\n0.8755216\r\n\r\n\r\n0e+00\r\n\r\n\r\n0.7620408\r\n\r\n\r\n0.7016995\r\n\r\n\r\n0.8755216\r\n\r\n\r\n1e-07\r\n\r\n\r\n0.7620408\r\n\r\n\r\n0.7016995\r\n\r\n\r\n0.8755216\r\n\r\n\r\n1e-06\r\n\r\n\r\n0.7620408\r\n\r\n\r\n0.7016995\r\n\r\n\r\n0.8755216\r\n\r\n\r\n1e-05\r\n\r\n\r\n0.7620408\r\n\r\n\r\n0.7016995\r\n\r\n\r\n0.8755216\r\n\r\n\r\n1e-04\r\n\r\n\r\n0.7620408\r\n\r\n\r\n0.7016995\r\n\r\n\r\n0.8755216\r\n\r\n\r\n1e-03\r\n\r\n\r\n0.8226939\r\n\r\n\r\n0.8125567\r\n\r\n\r\n0.8379148\r\n\r\n\r\n1e-02\r\n\r\n\r\n0.7982857\r\n\r\n\r\n0.8351836\r\n\r\n\r\n0.7149887\r\n\r\n\r\n1e-01\r\n\r\n\r\n0.8062857\r\n\r\n\r\n0.8723638\r\n\r\n\r\n0.6706259\r\n\r\n\r\nBoosted Trees Avg Accuracy = 0.7645666\r\nAll models seem to have high accuracies but Log Reg and Random Forest are roughly equal around 80%.\r\nWhen factoring in our desired outcome with outputted coefficients, and taking into account the much higher true positive rate in the log reg output, we have much more confidence in our selection of the logistic regression model and stand by our analysis as appropriate\r\nTakeaways\r\nBy cleaning play by play data, and creating summary statistics relating to each drive that occurred under two minutes, we were able to successfully analyze the importance of the 2-minute Drill in the game of football. Not only did we look into the players that are the “clutchest” under 2 minutes, but created our own model to correctly classify the result of a 2-minute drill drive. From there, we could observe the most important variable to determine what is the most important thing a team can do to end their drive with a score.\r\nWe show that QBR and number of penalties a team takes during the drive are the most important variables when it comes to team controllable variables. However, if you wanted to predict whether a team can end the drive in a score you should highly consider the number of yards the team has to drive.\r\nI think looking further into each 2 minute drill could lead to even more useful insights. One downfall of our own format is that we didn’t create enough variables to summarize each drive. There could be many more drive specific characteristics that could effect the models we created, such as if the team driving is the home or away team and the weather during the game. Nonetheless, we decided to focus only on the on-field aspects of the drive, especially Quarterback play.\r\nThank you so much for taking a look at my research! To take a further look at my source code, go to this Github link.\r\nData Ethics\r\nWe see no potential harm for this analysis. We were very open about how we performed our analysis with the data and goal of the project being benign in scope. We see no negative uses of this data and see it only having possible upside with more stats reliance. Better equipped teams and offenses will lead to more in-game action, giving NFL fans what they desire. The only negative impact may be upon defensive specialist players.\r\n\r\n\r\n\r\n",
    "preview": "Posts/2021-12-30-twominutedrill/Brady.jpg",
    "last_modified": "2021-12-30T16:51:40-05:00",
    "input_file": {}
  },
  {
    "path": "Posts/2021-11-04-ceterisparibus/",
    "title": "Ceteris Paribus Profile",
    "description": "My Lending Club Shiny app showing the probability of a loan getting fully paid back.",
    "author": [
      {
        "name": "Ben Wagner",
        "url": {}
      }
    ],
    "date": "2021-11-04",
    "categories": [],
    "contents": "\r\nThis is my Lending Club Shiny App! The app takes in multiple variables including Annual Income, Interest Rate, and Number of Accounts Delinquent in order to show the probability of the loan being paid back in full. This probability is found using a predictive random forest model which I created. Big thanks to Lisa Lendway for helping me with this Shiny app.\r\nIn order to make sure that the app was my own, I changed the color scheme and overall theme of the app. I used the “Lux” theme coming from the bslib library and made a color scheme of dark and light pink. If you would like to see the source code, check out my Github.\r\n\r\n\r\n\r\n",
    "preview": "Posts/2021-11-04-ceterisparibus/Shiny_app.jpg",
    "last_modified": "2021-11-04T20:15:36-04:00",
    "input_file": {}
  },
  {
    "path": "Posts/2021-10-26-ultratrailrunning/",
    "title": "Tidy Tuesday: Ultra Trail Running",
    "description": "My Tidy Tuesday Submission for Week 44",
    "author": [
      {
        "name": "Ben Wagner",
        "url": {}
      }
    ],
    "date": "2021-10-26",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n\r\n\r\n\r\nThis entry highlights my Tidy Tuesday submission for the week of October 26th! I created my vis using the Ultra Trail Running data provided by International Trail Running Association (ITRA)\r\nThis week, I wanted to highlight the winners of the races shown in the dataset. Thus, I filtered by rank 1. Before anything else, I wanted to see how many winners were from each nationality in the dataset, so I grouped by nationality and counted each occurence. I found that the United States, Great Britain, and France had the largest number of winners. So I filtered the dataset by only those 3 nationalities. From there I counted the number of occurences of each nationality at each age and plotted them on a line graph (grouped by nationality to show 3 different winner trends).\r\nI’d love to show my visualizations to as many people as possible, so your support is very much appreciated. If you’d like to see my source code, use this link to my Github !\r\n\r\n\r\n\r\n",
    "preview": "Posts/2021-10-26-ultratrailrunning/ultratrailrunning_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-11-04T20:11:56-04:00",
    "input_file": {}
  },
  {
    "path": "Posts/2021-10-20-tidytuesbigpumpkin/",
    "title": "Tidy Tuesday: Giant Watermelons (Oct. 20)",
    "description": "My Tidy Tuesday Submission for Week 43",
    "author": [
      {
        "name": "Ben Wagner",
        "url": {}
      }
    ],
    "date": "2021-10-20",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n\r\n\r\n\r\nThis entry highlights my Tidy Tuesday submission for the week of October 20! I created my vis using the Giant Pumpkins data provided by BigPumpkins.com\r\nThis weeks visualization highlights the states which harvested and submitted the greatest amount of watermelons in GPC’s competition. I initially wanted to show the states that harvested the most Giant Pumpkins, but I was finding too many missing observations for weight in pounds of each pumpkin. I first created a map containing all of the United States, then made sure each of the states listed in the Pumpkins dataset had the same spelling as my map. To find the amount of watermelons harvested in each state, I grouped the data by state and created a new variable containing the sum of each watermelon from that state. Finallly I created my ggplot map and filled each state by my new variable.\r\nIf you liked this week’s viz, please go like or even retweet on my twitter post. I’d love to show my visualizations to as many people as possible, so your support is very much appreciated. Also if you’d like to see my source code, use this link to my Github!\r\n\r\n\r\n\r\n",
    "preview": "Posts/2021-10-20-tidytuesbigpumpkin/watermelon.jpg",
    "last_modified": "2021-10-20T16:25:51-04:00",
    "input_file": {}
  },
  {
    "path": "Posts/2021-10-14-mlsshiny/",
    "title": "2021 MLS Visual Summary",
    "description": "A simple shiny app showing MLS team statistics for the 2021 season.",
    "author": [
      {
        "name": "Ben Wagner",
        "url": {}
      }
    ],
    "date": "2021-10-14",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nThis post highlights my Shiny app creation for my Advanced Data Science class. The app shows the user simple summary statistics of the 27 Major League Soccer teams during for the most recently season (2021). The dropdown menu allows the user to choose 1 of 11 stats, including Goals For, Expected Goals, or Expected Points. This data was provided by one of my favorite websites, American Soccer Analysis.\r\nI plan to continue to updgrade this app with other visual and user friendly features. However, for the sake of a deadline, I had to keep the app simple. Feel free to check in again in the next coming months to see the finished product. Here is the link to my Github to see the code for the app!\r\n\r\n\r\n\r\n",
    "preview": "Posts/2021-10-14-mlsshiny/MLS logos.jpg",
    "last_modified": "2021-10-14T16:25:09-04:00",
    "input_file": {}
  },
  {
    "path": "Posts/2021-10-05-emmytidytues/",
    "title": "Tidy Tuesday: Emmys (Oct. 5)",
    "description": "My Tidy Tuesday Submission for Week 41",
    "author": [
      {
        "name": "Ben Wagner",
        "url": {}
      }
    ],
    "date": "2021-10-05",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nThis entry highlights my Tidy Tuesday submission for the week of October 5! There was a problem with the latest dataset on Github, so I just resorted to an older file. Thus, I created my vis using the Emmy Awards data provided by emmys.com\r\nThis weeks visualization highlights the major distributors and the number of Emmys they win each year. I created an animated bar plot to show how many Emmys each group won from 2000-2021, excluding 2014 for a lack of data. To accomplish this goal, I first grouped the dataset by distributor and year, then counted the number of occurrences each distributor made in that year. I felt that an animation was beneficial, rather than faceting, because you can see the change in success as the age of streaming movies/TV has taken over the world.\r\nIf you liked this week’s viz, please go like or even retweet on my twitter post. I’d love to show my visualizations to as many people as possible, so your support is very much appreciated. Also if you’d like to see my source code, use this link to my Github!\r\n\r\n\r\n\r\n",
    "preview": "Posts/2021-10-05-emmytidytues/Emmy_logo.jpg",
    "last_modified": "2021-10-20T11:10:04-04:00",
    "input_file": {}
  },
  {
    "path": "Posts/2021-09-21-tidy-tuesday-billboard-top-100-sept-22/",
    "title": "Tidy Tuesday: Billboard Top 100 (Sept. 21)",
    "description": "My Tidy Tuesday submission for Week 39",
    "author": [
      {
        "name": "Ben Wagner",
        "url": {}
      }
    ],
    "date": "2021-09-21",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nI’m super excited to publish my first blog post on my website! This entry highlights my Tidy Tuesday submission for the week of September 21st. This week’s dataset contains information on the weekly Billboard Top 100 songs and each song’s audio features. The data comes from Data.World, scraped by Sean Miller on Billboard and Spotify.\r\nLooking at my visualization, I decided to show the relationship between the how long each #1 hit stayed at the top, and the level of danceability the song contains. People love to dance. The more we dance, the more we play those top hits. Thus, they continue to stay at the top for longer! In order to do this, I filtered out all the songs that didn’t peak at #1 and then created a new variable which counted the number of weeks it stayed at the top. From there, I created a scatter plot in order to show each song as a point. Also, it is easier to show the linear relationship (line of best fit) between the two variables. Finally, the color scheme seemed like the perfect aethstetically pleasing theme that makes it all pop.\r\nIf you liked my viz, please go like or even retweet on my twitter post. I’d love to show my visualizations to as many people as possible, so your support is very much appreciated. Also if you’d like to see my source code, use this link to my Github!\r\n\r\n\r\n\r\n",
    "preview": "Posts/2021-09-21-tidy-tuesday-billboard-top-100-sept-22/Tidy_Tuesday_Sept21.png",
    "last_modified": "2021-09-22T13:38:37-04:00",
    "input_file": {}
  }
]
